{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as Dataset\n",
    "import torchvision.transforms.functional as TVF\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import vgg16 \n",
    "from PIL import Image\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "import psutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.zeros(3,4)\n",
    "t1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=17046654976, available=10860032000, percent=36.3, used=6186622976, free=10860032000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0  1000268201_693b08cb0e.jpg#0   \n",
       "1  1000268201_693b08cb0e.jpg#1   \n",
       "2  1000268201_693b08cb0e.jpg#2   \n",
       "3  1000268201_693b08cb0e.jpg#3   \n",
       "4  1000268201_693b08cb0e.jpg#4   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['filename', 'caption']\n",
    "caption_data = pd.read_csv('./Flickr8k_text/Flickr8k_token.txt' , \n",
    "                           sep = '\\t' ,\n",
    "                          header = None,\n",
    "                          names = columns)\n",
    "caption_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>caption_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0  1000268201_693b08cb0e.jpg#0   \n",
       "1  1000268201_693b08cb0e.jpg#1   \n",
       "2  1000268201_693b08cb0e.jpg#2   \n",
       "3  1000268201_693b08cb0e.jpg#3   \n",
       "4  1000268201_693b08cb0e.jpg#4   \n",
       "\n",
       "                                             caption caption_num  \n",
       "0  A child in a pink dress is climbing up a set o...           0  \n",
       "1              A girl going into a wooden building .           1  \n",
       "2   A little girl climbing into a wooden playhouse .           2  \n",
       "3  A little girl climbing the stairs to her playh...           3  \n",
       "4  A little girl in a pink dress going into a woo...           4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data['caption_num'] = caption_data['filename'].str.split('#',expand=True)[1]\n",
    "caption_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40460, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40460 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40460.000000\n",
       "mean        11.782155\n",
       "std          3.885175\n",
       "min          1.000000\n",
       "25%          9.000000\n",
       "50%         11.000000\n",
       "75%         14.000000\n",
       "90%         17.000000\n",
       "95%         19.000000\n",
       "99%         23.000000\n",
       "max         38.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data['word_count'] = caption_data['caption'].apply(lambda x: len(str(x).split(\" \")))\n",
    "caption_data['word_count'].describe(percentiles = [0.25 , 0.5 , 0.75 , 0.9 , 0.95 , 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = 0\n",
    "word_to_idx_dict = {}\n",
    "idx_to_word_dict = {}\n",
    "sentence_list = []\n",
    "for i in caption_data['caption']:\n",
    "    text = i.lower().split(' ')\n",
    "    sentence_list.append(text)\n",
    "    for j in text:\n",
    "        if word_to_idx_dict.get(j,0) == 0:\n",
    "            word_to_idx_dict[j] = word_idx\n",
    "            idx_to_word_dict[word_idx] = j\n",
    "            word_idx+=1\n",
    "        else: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_idx_dict : 8918\n",
      "idx_to_word_dict : 8919\n"
     ]
    }
   ],
   "source": [
    "print('word_to_idx_dict : {}'.format(len(word_to_idx_dict)))\n",
    "print('idx_to_word_dict : {}'.format(len(idx_to_word_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patterns'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word_dict[8918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8918"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx_dict['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vgg16 = vgg16(pretrained=True)\n",
    "#model_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vgg16.classifier = nn.Linear(in_features=25088, out_features=4096, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file : 100 / 8091 in 0.36804676055908203 seconds\n",
      " file : 200 / 8091 in 0.3048670291900635 seconds\n",
      " file : 300 / 8091 in 0.3027181625366211 seconds\n",
      " file : 400 / 8091 in 0.3563570976257324 seconds\n",
      " file : 500 / 8091 in 0.2905843257904053 seconds\n",
      " file : 600 / 8091 in 0.3166837692260742 seconds\n",
      " file : 700 / 8091 in 0.31396484375 seconds\n",
      " file : 800 / 8091 in 0.32195329666137695 seconds\n",
      " file : 900 / 8091 in 0.29948925971984863 seconds\n",
      " file : 1000 / 8091 in 0.3249082565307617 seconds\n",
      " file : 1100 / 8091 in 0.31793689727783203 seconds\n",
      " file : 1200 / 8091 in 0.3161802291870117 seconds\n",
      " file : 1300 / 8091 in 0.3889596462249756 seconds\n",
      " file : 1400 / 8091 in 0.3112001419067383 seconds\n",
      " file : 1500 / 8091 in 0.3111598491668701 seconds\n",
      " file : 1600 / 8091 in 0.3111913204193115 seconds\n",
      " file : 1700 / 8091 in 0.30718326568603516 seconds\n",
      " file : 1800 / 8091 in 0.3022148609161377 seconds\n",
      " file : 1900 / 8091 in 0.3091914653778076 seconds\n",
      " file : 2000 / 8091 in 0.32016754150390625 seconds\n",
      " file : 2100 / 8091 in 0.32514047622680664 seconds\n",
      " file : 2200 / 8091 in 0.32014036178588867 seconds\n",
      " file : 2300 / 8091 in 0.3211650848388672 seconds\n",
      " file : 2400 / 8091 in 0.3191709518432617 seconds\n",
      " file : 2500 / 8091 in 0.32512450218200684 seconds\n",
      " file : 2600 / 8091 in 0.31518101692199707 seconds\n",
      " file : 2700 / 8091 in 0.35603928565979004 seconds\n",
      " file : 2800 / 8091 in 0.320159912109375 seconds\n",
      " file : 2900 / 8091 in 0.3241305351257324 seconds\n",
      " file : 3000 / 8091 in 0.5006864070892334 seconds\n",
      " file : 3100 / 8091 in 0.5066537857055664 seconds\n",
      " file : 3200 / 8091 in 0.5226008892059326 seconds\n",
      " file : 3300 / 8091 in 0.4817378520965576 seconds\n",
      " file : 3400 / 8091 in 0.45975542068481445 seconds\n",
      " file : 3500 / 8091 in 0.34308362007141113 seconds\n",
      " file : 3600 / 8091 in 0.4465827941894531 seconds\n",
      " file : 3700 / 8091 in 0.34779787063598633 seconds\n",
      " file : 3800 / 8091 in 0.3498358726501465 seconds\n",
      " file : 3900 / 8091 in 0.3389866352081299 seconds\n",
      " file : 4000 / 8091 in 0.4906768798828125 seconds\n",
      " file : 4100 / 8091 in 0.4976975917816162 seconds\n",
      " file : 4200 / 8091 in 0.4707305431365967 seconds\n",
      " file : 4300 / 8091 in 0.32313013076782227 seconds\n",
      " file : 4400 / 8091 in 0.3201308250427246 seconds\n",
      " file : 4500 / 8091 in 0.3191392421722412 seconds\n",
      " file : 4600 / 8091 in 0.3131523132324219 seconds\n",
      " file : 4700 / 8091 in 0.4687178134918213 seconds\n",
      " file : 4800 / 8091 in 0.351085901260376 seconds\n",
      " file : 4900 / 8091 in 0.3229339122772217 seconds\n",
      " file : 5000 / 8091 in 0.3141822814941406 seconds\n",
      " file : 5100 / 8091 in 0.309171199798584 seconds\n",
      " file : 5200 / 8091 in 0.3161797523498535 seconds\n",
      " file : 5300 / 8091 in 0.3220787048339844 seconds\n",
      " file : 5400 / 8091 in 0.31213903427124023 seconds\n",
      " file : 5500 / 8091 in 0.3161442279815674 seconds\n",
      " file : 5600 / 8091 in 0.3301424980163574 seconds\n",
      " file : 5700 / 8091 in 0.32515621185302734 seconds\n",
      " file : 5800 / 8091 in 0.31812310218811035 seconds\n",
      " file : 5900 / 8091 in 0.3321242332458496 seconds\n",
      " file : 6000 / 8091 in 0.3193802833557129 seconds\n",
      " file : 6100 / 8091 in 0.3211669921875 seconds\n",
      " file : 6200 / 8091 in 0.32715344429016113 seconds\n",
      " file : 6300 / 8091 in 0.31818580627441406 seconds\n",
      " file : 6400 / 8091 in 0.3231511116027832 seconds\n",
      " file : 6500 / 8091 in 0.32216405868530273 seconds\n",
      " file : 6600 / 8091 in 0.32714033126831055 seconds\n",
      " file : 6700 / 8091 in 0.3271493911743164 seconds\n",
      " file : 6800 / 8091 in 0.3233802318572998 seconds\n",
      " file : 6900 / 8091 in 0.3321115970611572 seconds\n",
      " file : 7000 / 8091 in 0.3340926170349121 seconds\n",
      " file : 7100 / 8091 in 0.3318173885345459 seconds\n",
      " file : 7200 / 8091 in 0.3181493282318115 seconds\n",
      " file : 7300 / 8091 in 0.3310587406158447 seconds\n",
      " file : 7400 / 8091 in 0.32615232467651367 seconds\n",
      " file : 7500 / 8091 in 0.3471033573150635 seconds\n",
      " file : 7600 / 8091 in 0.34709739685058594 seconds\n",
      " file : 7700 / 8091 in 0.33512115478515625 seconds\n",
      " file : 7800 / 8091 in 0.338120698928833 seconds\n",
      " file : 7900 / 8091 in 0.3231668472290039 seconds\n",
      " file : 8000 / 8091 in 0.3819739818572998 seconds\n"
     ]
    }
   ],
   "source": [
    "p = torchvision.transforms.Compose([torchvision.transforms.Resize((100,75))])\n",
    "image_dict = {}\n",
    "ctr = 0\n",
    "path =  'D:\\\\Data_Science\\\\image_captioning\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset'\n",
    "total_files = len(os.listdir(path))\n",
    "st_time = time.time()\n",
    "for file in os.listdir(path):\n",
    "    image = Image.open(path + '\\\\' + file)\n",
    "    image = p(image)\n",
    "    image_dict[file] = TVF.to_tensor(image)\n",
    "    ctr+=1\n",
    "    if ctr % 100 == 0:\n",
    "        print(' file : {} / {} in {} seconds'.format(ctr , total_files , (time.time() - st_time)))\n",
    "        st_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 38\n",
    "vocab_size = len(word_to_idx_dict)\n",
    "embed_size = 4096\n",
    "hidden_size = embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        # get the pretrained vgg16 model\n",
    "        self.vgg16 = vgg16(pretrained=True)\n",
    "        \n",
    "        # replace the classifier with a fully connected layer\n",
    "        self.vgg16.classifier = nn.Linear(in_features=25088, out_features=embed_size, bias=True)\n",
    "        \n",
    "        # add another fully connected layer\n",
    "        self.fc = nn.Linear(in_features=embed_size, out_features=embed_size)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get the embeddings from vgg16\n",
    "        x = self.vgg16(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # pass through dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # pass through the fully connected\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "    \n",
    "    def forward(self, embedding, caption_embed):\n",
    "        \n",
    "        outputs = torch.empty((batch_size , caption_embed.size(1) , vocab_size))\n",
    "        hidden_state = torch.zeros((batch_size, hidden_size)).to(device)\n",
    "        cell_state = torch.zeros((batch_size, hidden_size)).to(device)\n",
    "        \n",
    "        # pass the caption word by word\n",
    "        for t in range(caption_embed.size(1)):\n",
    "\n",
    "            # for the first time step the input is the embedding vector\n",
    "            if t == 0:\n",
    "                hidden_state , cell_state = self.lstm_cell(embedding , (hidden_state, cell_state))\n",
    "                \n",
    "            #fot t>0 , input is previous hidden state and cell states\n",
    "            else :\n",
    "                hidden_state, cell_state = self.lstm_cell(hidden_state, (hidden_state, cell_state))\n",
    "            \n",
    "            #Pass through output layer to match vocab size\n",
    "            out = self.fc_out(hidden_state)\n",
    "            \n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = EncoderCNN(embed_size = embed_size)\n",
    "decoder_rnn = DecoderRNN(embed_size = embed_size , hidden_size = embed_size , vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "    device = torch.device(\"cuda\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-3531c1353550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcaption_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaption_embed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch = 0\n",
    "print_every = 1\n",
    "batch_size = 1\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "encoder_rnn.to(device)\n",
    "decoder_rnn.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for df_idx in caption_data.head().index:\n",
    "        file_name = caption_data.iloc[df_idx].filename.split('#')[0]\n",
    "        print(file_name)\n",
    "        \n",
    "        image = image_dict[file_name]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embedding = encoder_rnn(image)\n",
    "    \n",
    "        caption = caption_data.iloc[0].caption\n",
    "        caption = caption.lower().split(' ')\n",
    "        \n",
    "        # init the hidden and cell states to zeros\n",
    "        caption_embed = torch.zeros((batch_size ,len(caption) , vocab_size) ).to(device)\n",
    "        \n",
    "        idx = 0\n",
    "        batch = 0\n",
    "        for words in caption:\n",
    "            word_idx = word_to_idx_dict[words]\n",
    "            caption_embed[batch][idx][word_idx] = 1\n",
    "            idx+=1\n",
    "        \n",
    "        y_pred = decoder_rnn(embedding = embedding , caption_embed = caption_embed)\n",
    "        loss = \n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if batch % print_every == 0:\n",
    "            print('epoch : {} \\t batch number : {} \\t train loss : {}'.format(epoch, batch,loss.item()))\n",
    "            \n",
    "        batch +=1\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = encoder_rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = caption_data.iloc[0].caption\n",
    "caption = caption.lower().split(' ')\n",
    "        \n",
    "# init the hidden and cell states to zeros\n",
    "caption_embed = torch.zeros((batch_size ,len(caption) , vocab_size) ).to(device)\n",
    "        \n",
    "idx = 0\n",
    "batch = 0\n",
    "for words in caption:\n",
    "    word_idx = word_to_idx_dict[words]\n",
    "    caption_embed[batch][idx][word_idx] = 1\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decoder_rnn(embedding = embedding , caption_embed = caption_embed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 8918])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_size = embedding.shape[1]\n",
    "decoder_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 8918])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 8918])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(y_pred , caption_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = nn.LSTMCell(input_size=decoder_input_size, hidden_size=decoder_input_size)\n",
    "fc_out = nn.Linear(in_features=decoder_input_size, out_features=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden_state = torch.zeros((batch_size , decoder_input_size))\n",
    "cell_state = torch.zeros((batch_size , decoder_input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state , cell_state = lstm_cell(embedding , ((hidden_state, cell_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fc_out(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8918])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pinkish'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word_dict[torch.argmax(out.softmax(dim = 1)).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'child',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pink',\n",
       " 'dress',\n",
       " 'is',\n",
       " 'climbing',\n",
       " 'up',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'stairs',\n",
       " 'in',\n",
       " 'an',\n",
       " 'entry',\n",
       " 'way',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_caption = caption_data.iloc[0].caption\n",
    "predicted_sentence = []\n",
    "actual_caption = actual_caption.lower().split(' ')\n",
    "actual_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t :  0\tword : expressions\n",
      "t :  1\tword : expressions\n",
      "t :  2\tword : baptized\n",
      "t :  3\tword : opposition\n",
      "t :  4\tword : opposition\n",
      "t :  5\tword : bras\n",
      "t :  6\tword : bras\n",
      "t :  7\tword : bras\n",
      "t :  8\tword : bras\n",
      "t :  9\tword : ceremonial\n",
      "t :  10\tword : ceremonial\n",
      "t :  11\tword : ceremonial\n",
      "t :  12\tword : ceremonial\n",
      "t :  13\tword : ceremonial\n",
      "t :  14\tword : ceremonial\n",
      "t :  15\tword : ceremonial\n",
      "t :  16\tword : ceremonial\n",
      "t :  17\tword : ceremonial\n"
     ]
    }
   ],
   "source": [
    "for t in range(len(actual_caption)):\n",
    "    if t == 0:\n",
    "        hidden_state , cell_state = lstm_cell(embedding , ((hidden_state, cell_state)))\n",
    "    else :\n",
    "        hidden_state, cell_state = lstm_cell(hidden_state, (hidden_state, cell_state))\n",
    "        \n",
    "    out = fc_out(hidden_state)\n",
    "    word = idx_to_word_dict[torch.argmax(out.softmax(dim = 1)).item()]\n",
    "    predicted_sentence.append(word)\n",
    "    print('t :  {}\\tword : {}'.format(t,word))\n",
    "        \n",
    "#predicted_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tensor = torch.zeros(8918)\n",
    "word_idx = word_to_idx_dict['a']\n",
    "word_tensor = zero_tensor\n",
    "word_tensor[word_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 8918])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embed = torch.zeros((batch_size ,len(actual_caption) , vocab_size) )\n",
    "sentence_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "batch = 0\n",
    "for words in actual_caption:\n",
    "    word_idx = word_to_idx_dict[words]\n",
    "    sentence_embed[batch][idx][word_idx] = 1\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 8918])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.Flickr8k(root = 'D:/Data_Science/image_captioning/Flickr8k_Dataset/Flickr8k_Dataset',\n",
    "                                             ann_file = 'D:/Data_Science/image_captioning/Flickr8k_text/Flickr8k_token.txt',\n",
    "                                             transform = torchvision.transforms.ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_loader = torch.utils.data.DataLoader(train_set,batch_size=4,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-3d71cbddeefb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "var = iter(next(train_set_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-5c7063cefad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iter(train_set_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file : 100 / 8091 in 0.30092740058898926 seconds\n",
      " file : 200 / 8091 in 0.31882214546203613 seconds\n",
      " file : 300 / 8091 in 0.2999558448791504 seconds\n",
      " file : 400 / 8091 in 0.33214545249938965 seconds\n",
      " file : 500 / 8091 in 0.2991297245025635 seconds\n",
      " file : 600 / 8091 in 0.31565427780151367 seconds\n",
      " file : 700 / 8091 in 0.3010292053222656 seconds\n",
      " file : 800 / 8091 in 0.31662845611572266 seconds\n",
      " file : 900 / 8091 in 0.32251906394958496 seconds\n",
      " file : 1000 / 8091 in 0.31117987632751465 seconds\n",
      " file : 1100 / 8091 in 0.31917476654052734 seconds\n",
      " file : 1200 / 8091 in 0.2930927276611328 seconds\n",
      " file : 1300 / 8091 in 0.2999758720397949 seconds\n",
      " file : 1400 / 8091 in 0.3000307083129883 seconds\n",
      " file : 1500 / 8091 in 0.301058292388916 seconds\n",
      " file : 1600 / 8091 in 0.2999439239501953 seconds\n",
      " file : 1700 / 8091 in 0.3009648323059082 seconds\n",
      " file : 1800 / 8091 in 0.298907995223999 seconds\n",
      " file : 1900 / 8091 in 0.31517505645751953 seconds\n",
      " file : 2000 / 8091 in 0.32416534423828125 seconds\n",
      " file : 2100 / 8091 in 0.30859971046447754 seconds\n",
      " file : 2200 / 8091 in 0.31581950187683105 seconds\n",
      " file : 2300 / 8091 in 0.3160519599914551 seconds\n",
      " file : 2400 / 8091 in 0.3172152042388916 seconds\n",
      " file : 2500 / 8091 in 0.31668758392333984 seconds\n",
      " file : 2600 / 8091 in 0.3166515827178955 seconds\n",
      " file : 2700 / 8091 in 0.31559181213378906 seconds\n",
      " file : 2800 / 8091 in 0.31668782234191895 seconds\n",
      " file : 2900 / 8091 in 0.3165853023529053 seconds\n",
      " file : 3000 / 8091 in 0.31663966178894043 seconds\n",
      " file : 3100 / 8091 in 0.330003023147583 seconds\n",
      " file : 3200 / 8091 in 0.31818342208862305 seconds\n",
      " file : 3300 / 8091 in 0.32315731048583984 seconds\n",
      " file : 3400 / 8091 in 0.2872025966644287 seconds\n",
      " file : 3500 / 8091 in 0.3212251663208008 seconds\n",
      " file : 3600 / 8091 in 0.3166465759277344 seconds\n",
      " file : 3700 / 8091 in 0.44603991508483887 seconds\n",
      " file : 3800 / 8091 in 0.37512636184692383 seconds\n",
      " file : 3900 / 8091 in 0.31841611862182617 seconds\n",
      " file : 4000 / 8091 in 0.3406062126159668 seconds\n",
      " file : 4100 / 8091 in 0.3020625114440918 seconds\n",
      " file : 4200 / 8091 in 0.3166680335998535 seconds\n",
      " file : 4300 / 8091 in 0.29993152618408203 seconds\n",
      " file : 4400 / 8091 in 0.3010857105255127 seconds\n",
      " file : 4500 / 8091 in 0.2998640537261963 seconds\n",
      " file : 4600 / 8091 in 0.2854750156402588 seconds\n",
      " file : 4700 / 8091 in 0.29996347427368164 seconds\n",
      " file : 4800 / 8091 in 0.29993581771850586 seconds\n",
      " file : 4900 / 8091 in 0.30010247230529785 seconds\n",
      " file : 5000 / 8091 in 0.2999379634857178 seconds\n",
      " file : 5100 / 8091 in 0.31662821769714355 seconds\n",
      " file : 5200 / 8091 in 0.2999756336212158 seconds\n",
      " file : 5300 / 8091 in 0.3159821033477783 seconds\n",
      " file : 5400 / 8091 in 0.30167508125305176 seconds\n",
      " file : 5500 / 8091 in 0.3000185489654541 seconds\n",
      " file : 5600 / 8091 in 0.3155961036682129 seconds\n",
      " file : 5700 / 8091 in 0.3166508674621582 seconds\n",
      " file : 5800 / 8091 in 0.3165755271911621 seconds\n",
      " file : 5900 / 8091 in 0.33184361457824707 seconds\n",
      " file : 6000 / 8091 in 0.31628847122192383 seconds\n",
      " file : 6100 / 8091 in 0.32215261459350586 seconds\n",
      " file : 6200 / 8091 in 0.3063030242919922 seconds\n",
      " file : 6300 / 8091 in 0.31963086128234863 seconds\n",
      " file : 6400 / 8091 in 0.2994248867034912 seconds\n",
      " file : 6500 / 8091 in 0.3166065216064453 seconds\n",
      " file : 6600 / 8091 in 0.34237003326416016 seconds\n",
      " file : 6700 / 8091 in 0.372011661529541 seconds\n",
      " file : 6800 / 8091 in 0.39447712898254395 seconds\n",
      " file : 6900 / 8091 in 0.2954132556915283 seconds\n",
      " file : 7000 / 8091 in 0.3354930877685547 seconds\n",
      " file : 7100 / 8091 in 0.3176915645599365 seconds\n",
      " file : 7200 / 8091 in 0.31668615341186523 seconds\n",
      " file : 7300 / 8091 in 0.31665635108947754 seconds\n",
      " file : 7400 / 8091 in 0.33324241638183594 seconds\n",
      " file : 7500 / 8091 in 0.3322930335998535 seconds\n",
      " file : 7600 / 8091 in 0.3332509994506836 seconds\n",
      " file : 7700 / 8091 in 0.317673921585083 seconds\n",
      " file : 7800 / 8091 in 0.3333909511566162 seconds\n",
      " file : 7900 / 8091 in 0.3165717124938965 seconds\n",
      " file : 8000 / 8091 in 0.3010268211364746 seconds\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "375 / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "375 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg#0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data.filename[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
