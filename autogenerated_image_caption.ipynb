{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as Dataset\n",
    "import torchvision.transforms.functional as TVF\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import vgg16 , inception_v3 , resnet50\n",
    "from PIL import Image\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "import psutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.zeros(3,4)\n",
    "t1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=17046654976, available=9272930304, percent=45.6, used=7773724672, free=9272930304)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0  1000268201_693b08cb0e.jpg#0   \n",
       "1  1000268201_693b08cb0e.jpg#1   \n",
       "2  1000268201_693b08cb0e.jpg#2   \n",
       "3  1000268201_693b08cb0e.jpg#3   \n",
       "4  1000268201_693b08cb0e.jpg#4   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['filename', 'caption']\n",
    "caption_data = pd.read_csv('./Flickr8k_text/Flickr8k_token.txt' , \n",
    "                           sep = '\\t' ,\n",
    "                          header = None,\n",
    "                          names = columns)\n",
    "caption_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>caption_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0  1000268201_693b08cb0e.jpg#0   \n",
       "1  1000268201_693b08cb0e.jpg#1   \n",
       "2  1000268201_693b08cb0e.jpg#2   \n",
       "3  1000268201_693b08cb0e.jpg#3   \n",
       "4  1000268201_693b08cb0e.jpg#4   \n",
       "\n",
       "                                             caption caption_num  \n",
       "0  A child in a pink dress is climbing up a set o...           0  \n",
       "1              A girl going into a wooden building .           1  \n",
       "2   A little girl climbing into a wooden playhouse .           2  \n",
       "3  A little girl climbing the stairs to her playh...           3  \n",
       "4  A little girl in a pink dress going into a woo...           4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data['caption_num'] = caption_data['filename'].str.split('#',expand=True)[1]\n",
    "caption_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40460, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40460 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40460.000000\n",
       "mean        11.782155\n",
       "std          3.885175\n",
       "min          1.000000\n",
       "25%          9.000000\n",
       "50%         11.000000\n",
       "75%         14.000000\n",
       "90%         17.000000\n",
       "95%         19.000000\n",
       "99%         23.000000\n",
       "max         38.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_data['word_count'] = caption_data['caption'].apply(lambda x: len(str(x).split(\" \")))\n",
    "caption_data['word_count'].describe(percentiles = [0.25 , 0.5 , 0.75 , 0.9 , 0.95 , 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = 0\n",
    "word_to_idx_dict = {}\n",
    "idx_to_word_dict = {}\n",
    "sentence_list = []\n",
    "for i in caption_data['caption']:\n",
    "    text = i.lower().split(' ')\n",
    "    sentence_list.append(text)\n",
    "    for j in text:\n",
    "        if word_to_idx_dict.get(j,0) == 0:\n",
    "            word_to_idx_dict[j] = word_idx\n",
    "            idx_to_word_dict[word_idx] = j\n",
    "            word_idx+=1\n",
    "        else: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_idx_dict : 8918\n",
      "idx_to_word_dict : 8919\n"
     ]
    }
   ],
   "source": [
    "print('word_to_idx_dict : {}'.format(len(word_to_idx_dict)))\n",
    "print('idx_to_word_dict : {}'.format(len(idx_to_word_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patterns'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word_dict[8918]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8918"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx_dict['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vgg16 = vgg16(pretrained=True)\n",
    "#model_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vgg16.classifier = nn.Linear(in_features=25088, out_features=4096, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file : 100 / 8091 in 0.4971456527709961 seconds\n",
      " file : 200 / 8091 in 0.5387814044952393 seconds\n",
      " file : 300 / 8091 in 0.5374798774719238 seconds\n",
      " file : 400 / 8091 in 0.5716202259063721 seconds\n",
      " file : 500 / 8091 in 0.5245392322540283 seconds\n",
      " file : 600 / 8091 in 0.5654861927032471 seconds\n",
      " file : 700 / 8091 in 0.5299861431121826 seconds\n",
      " file : 800 / 8091 in 0.5591206550598145 seconds\n",
      " file : 900 / 8091 in 0.543346643447876 seconds\n",
      " file : 1000 / 8091 in 0.4929971694946289 seconds\n",
      " file : 1100 / 8091 in 0.5432205200195312 seconds\n",
      " file : 1200 / 8091 in 0.5275876522064209 seconds\n",
      " file : 1300 / 8091 in 0.511976957321167 seconds\n",
      " file : 1400 / 8091 in 0.5286915302276611 seconds\n",
      " file : 1500 / 8091 in 0.5104763507843018 seconds\n",
      " file : 1600 / 8091 in 0.5081908702850342 seconds\n",
      " file : 1700 / 8091 in 0.5837013721466064 seconds\n",
      " file : 1800 / 8091 in 0.5308177471160889 seconds\n",
      " file : 1900 / 8091 in 0.5218682289123535 seconds\n",
      " file : 2000 / 8091 in 0.5543076992034912 seconds\n",
      " file : 2100 / 8091 in 0.7361457347869873 seconds\n",
      " file : 2200 / 8091 in 0.5451912879943848 seconds\n",
      " file : 2300 / 8091 in 0.5425479412078857 seconds\n",
      " file : 2400 / 8091 in 0.5155439376831055 seconds\n",
      " file : 2500 / 8091 in 0.5335724353790283 seconds\n",
      " file : 2600 / 8091 in 0.5119993686676025 seconds\n",
      " file : 2700 / 8091 in 0.5532753467559814 seconds\n",
      " file : 2800 / 8091 in 0.5749456882476807 seconds\n",
      " file : 2900 / 8091 in 0.5395612716674805 seconds\n",
      " file : 3000 / 8091 in 0.5265908241271973 seconds\n",
      " file : 3100 / 8091 in 0.5539071559906006 seconds\n",
      " file : 3200 / 8091 in 0.5146327018737793 seconds\n",
      " file : 3300 / 8091 in 0.5249204635620117 seconds\n",
      " file : 3400 / 8091 in 0.5554556846618652 seconds\n",
      " file : 3500 / 8091 in 0.529346227645874 seconds\n",
      " file : 3600 / 8091 in 0.9125607013702393 seconds\n",
      " file : 3700 / 8091 in 0.5168459415435791 seconds\n",
      " file : 3800 / 8091 in 0.6060614585876465 seconds\n",
      " file : 3900 / 8091 in 0.5763161182403564 seconds\n",
      " file : 4000 / 8091 in 0.5550308227539062 seconds\n",
      " file : 4100 / 8091 in 0.6061079502105713 seconds\n",
      " file : 4200 / 8091 in 0.5263035297393799 seconds\n",
      " file : 4300 / 8091 in 0.5994527339935303 seconds\n",
      " file : 4400 / 8091 in 0.5216035842895508 seconds\n",
      " file : 4500 / 8091 in 0.5070171356201172 seconds\n",
      " file : 4600 / 8091 in 0.7259016036987305 seconds\n",
      " file : 4700 / 8091 in 0.5749397277832031 seconds\n",
      " file : 4800 / 8091 in 0.5773956775665283 seconds\n",
      " file : 4900 / 8091 in 0.5106346607208252 seconds\n",
      " file : 5000 / 8091 in 0.5149979591369629 seconds\n",
      " file : 5100 / 8091 in 0.5438840389251709 seconds\n",
      " file : 5200 / 8091 in 0.5511035919189453 seconds\n",
      " file : 5300 / 8091 in 0.5322256088256836 seconds\n",
      " file : 5400 / 8091 in 0.537992000579834 seconds\n",
      " file : 5500 / 8091 in 0.5515260696411133 seconds\n",
      " file : 5600 / 8091 in 0.55112624168396 seconds\n",
      " file : 5700 / 8091 in 0.5580873489379883 seconds\n",
      " file : 5800 / 8091 in 0.5535304546356201 seconds\n",
      " file : 5900 / 8091 in 0.5800983905792236 seconds\n",
      " file : 6000 / 8091 in 0.5335748195648193 seconds\n",
      " file : 6100 / 8091 in 0.5589685440063477 seconds\n",
      " file : 6200 / 8091 in 0.5948243141174316 seconds\n",
      " file : 6300 / 8091 in 0.5661771297454834 seconds\n",
      " file : 6400 / 8091 in 0.551520824432373 seconds\n",
      " file : 6500 / 8091 in 0.5488896369934082 seconds\n",
      " file : 6600 / 8091 in 0.7231173515319824 seconds\n",
      " file : 6700 / 8091 in 0.6531205177307129 seconds\n",
      " file : 6800 / 8091 in 1.65440034866333 seconds\n",
      " file : 6900 / 8091 in 1.3280808925628662 seconds\n",
      " file : 7000 / 8091 in 1.2921240329742432 seconds\n",
      " file : 7100 / 8091 in 2.1486310958862305 seconds\n",
      " file : 7200 / 8091 in 1.4313347339630127 seconds\n",
      " file : 7300 / 8091 in 1.4496703147888184 seconds\n",
      " file : 7400 / 8091 in 1.496631383895874 seconds\n",
      " file : 7500 / 8091 in 1.4742815494537354 seconds\n",
      " file : 7600 / 8091 in 2.1985297203063965 seconds\n",
      " file : 7700 / 8091 in 1.400190830230713 seconds\n",
      " file : 7800 / 8091 in 2.0273385047912598 seconds\n",
      " file : 7900 / 8091 in 2.308401346206665 seconds\n",
      " file : 8000 / 8091 in 2.2544524669647217 seconds\n"
     ]
    }
   ],
   "source": [
    "p = torchvision.transforms.Compose([torchvision.transforms.Resize((299,299))])\n",
    "image_dict = {}\n",
    "ctr = 0\n",
    "path =  'D:\\\\Data_Science\\\\image_captioning\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset'\n",
    "total_files = len(os.listdir(path))\n",
    "st_time = time.time()\n",
    "for file in os.listdir(path):\n",
    "    image = Image.open(path + '\\\\' + file)\n",
    "    image = p(image)\n",
    "    image_dict[file] = TVF.to_tensor(image)\n",
    "    ctr+=1\n",
    "    if ctr % 100 == 0:\n",
    "        print(' file : {} / {} in {} seconds'.format(ctr , total_files , (time.time() - st_time)))\n",
    "        st_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 38\n",
    "vocab_size = len(word_to_idx_dict)\n",
    "embed_size = 4096\n",
    "hidden_size = embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 299, 299])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic = resnet50(pretrained=True)\n",
    "ic.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = ic(image)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        # get the pretrained vgg16 model\n",
    "        #self.vgg16 = vgg16(pretrained=True)\n",
    "        self.resnet50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # replace the classifier with a fully connected layer\n",
    "        #self.vgg16.classifier = nn.Linear(in_features=25088, out_features=embed_size, bias=True)\n",
    "        self.resnet50.fc = nn.Linear(in_features=2048, out_features=embed_size, bias=True)\n",
    "        \n",
    "        # add another fully connected layer\n",
    "        self.fc = nn.Linear(in_features=embed_size, out_features=embed_size)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get the embeddings from vgg16\n",
    "        x = self.resnet50(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # pass through dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # pass through the fully connected\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "    \n",
    "    def forward(self, embedding, caption_embed):\n",
    "        \n",
    "        outputs = torch.empty((batch_size , caption_embed.size(1) , vocab_size))\n",
    "        hidden_state = torch.zeros((batch_size, hidden_size)).to(device)\n",
    "        cell_state = torch.zeros((batch_size, hidden_size)).to(device)\n",
    "        \n",
    "        # pass the caption word by word\n",
    "        for t in range(caption_embed.size(1)):\n",
    "\n",
    "            # for the first time step the input is the embedding vector\n",
    "            if t == 0:\n",
    "                hidden_state , cell_state = self.lstm_cell(embedding , (hidden_state, cell_state))\n",
    "                \n",
    "            #fot t>0 , input is previous hidden state and cell states\n",
    "            else :\n",
    "                hidden_state, cell_state = self.lstm_cell(hidden_state, (hidden_state, cell_state))\n",
    "            \n",
    "            #Pass through output layer to match vocab size\n",
    "            out = self.fc_out(hidden_state)\n",
    "            \n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = EncoderCNN(embed_size = embed_size)\n",
    "decoder_rnn = DecoderRNN(embed_size = embed_size , hidden_size = embed_size , vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "    device = torch.device(\"cuda\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 \t record number : 0 / 40460 \t train loss : 0.00017897880752570927\n",
      "epoch : 0 \t record number : 1 / 40460 \t train loss : 0.00014989908959250897\n",
      "epoch : 0 \t record number : 2 / 40460 \t train loss : 0.00012928545766044408\n",
      "epoch : 0 \t record number : 3 / 40460 \t train loss : 0.00011706171790137887\n",
      "epoch : 0 \t record number : 4 / 40460 \t train loss : 0.00011699085735017434\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "ctr = 0\n",
    "print_every = 1\n",
    "batch_size = 1\n",
    "total_records = caption_data.shape[0]\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(decoder_rnn.parameters(), lr = learning_rate)\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "encoder_rnn.to(device)\n",
    "decoder_rnn.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for df_idx in caption_data.head().index:\n",
    "        file_name = caption_data.iloc[df_idx].filename.split('#')[0]\n",
    "        #print(file_name)\n",
    "        \n",
    "        image = image_dict[file_name]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embedding = encoder_rnn(image)\n",
    "    \n",
    "        caption = caption_data.iloc[0].caption\n",
    "        caption = caption.lower().split(' ')\n",
    "        \n",
    "        # init the hidden and cell states to zeros\n",
    "        caption_embed = torch.zeros((batch_size ,len(caption) , vocab_size) ).to(device)\n",
    "        \n",
    "        idx = 0\n",
    "        batch = 0\n",
    "        for words in caption:\n",
    "            word_idx = word_to_idx_dict[words]\n",
    "            caption_embed[batch][idx][word_idx] = 1\n",
    "            idx+=1\n",
    "        \n",
    "        y_pred = decoder_rnn(embedding = embedding , caption_embed = caption_embed)\n",
    "        loss = loss_fn(y_pred , caption_embed) \n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if ctr % print_every == 0:\n",
    "            print('epoch : {} \\t record number : {} / {} \\t train loss : {}'.format(epoch, ctr ,total_records,loss.item()))\n",
    "            \n",
    "        ctr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
